{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fa873a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/mdepres/graph_neural_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a0b2995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 7, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
      "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (4/4), 334 bytes | 55.00 KiB/s, done.\n",
      "From https://github.com/mdepres/graph_neural_net\n",
      "   56252dd..0313746  main       -> origin/main\n",
      "Updating 56252dd..0313746\n",
      "Fast-forward\n",
      " models/finetuning_models.py | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " 1 file changed, 1 insertion(+), 1 deletion(-)\n"
     ]
    }
   ],
   "source": [
    "#!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aad275f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/graph_neural_net/graph_neural_net\n",
      "commander_explore.py\t\t  LICENSE\t\t       qap\n",
      "data_benchmarking_gnns\t\t  loaders\t\t       README.md\n",
      "default_config.yaml\t\t  maskedtensors\t\t       requirements.txt\n",
      "finetune_FGNN_for_coloring.ipynb  models\t\t       toolbox\n",
      "images\t\t\t\t  plot_accuracy_regular.ipynb\n"
     ]
    }
   ],
   "source": [
    "#%cd graph_neural_net/\n",
    "#!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e38247f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=1.5 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.6.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (2.8.4)\n",
      "Requirement already satisfied: pytest in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (6.1.1)\n",
      "Requirement already satisfied: sacred in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (0.8.2)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (6.0)\n",
      "Requirement already satisfied: neptune-contrib in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (0.28.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (5.9.0)\n",
      "Requirement already satisfied: Cython in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (0.29.30)\n",
      "Requirement already satisfied: sklearn in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (0.0.post5)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (1.1.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 12)) (4.64.0)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 13)) (0.11.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 14)) (1.2.5)\n",
      "Requirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 15)) (2.0.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.5->-r requirements.txt (line 1)) (4.3.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.8/site-packages (from torch>=1.5->-r requirements.txt (line 1)) (10.9.0.58)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from torch>=1.5->-r requirements.txt (line 1)) (3.7.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from torch>=1.5->-r requirements.txt (line 1)) (3.0.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.8/site-packages (from torch>=1.5->-r requirements.txt (line 1)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.8/site-packages (from torch>=1.5->-r requirements.txt (line 1)) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.8/site-packages (from torch>=1.5->-r requirements.txt (line 1)) (11.7.99)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.8/site-packages (from torch>=1.5->-r requirements.txt (line 1)) (1.7.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.8/site-packages (from torch>=1.5->-r requirements.txt (line 1)) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.8/site-packages (from torch>=1.5->-r requirements.txt (line 1)) (11.4.0.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.8/site-packages (from torch>=1.5->-r requirements.txt (line 1)) (2.0.0)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.8/site-packages (from torch>=1.5->-r requirements.txt (line 1)) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.8/site-packages (from torch>=1.5->-r requirements.txt (line 1)) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.8/site-packages (from torch>=1.5->-r requirements.txt (line 1)) (11.7.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.8/site-packages (from torch>=1.5->-r requirements.txt (line 1)) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.8/site-packages (from torch>=1.5->-r requirements.txt (line 1)) (11.10.3.66)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.5->-r requirements.txt (line 1)) (59.8.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.5->-r requirements.txt (line 1)) (0.37.1)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.5->-r requirements.txt (line 1)) (16.0.5)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.5->-r requirements.txt (line 1)) (3.26.3)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /opt/conda/lib/python3.8/site-packages (from scipy->-r requirements.txt (line 2)) (1.22.1)\n",
      "Requirement already satisfied: py>=1.8.2 in /opt/conda/lib/python3.8/site-packages (from pytest->-r requirements.txt (line 4)) (1.11.0)\n",
      "Requirement already satisfied: iniconfig in /opt/conda/lib/python3.8/site-packages (from pytest->-r requirements.txt (line 4)) (1.1.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from pytest->-r requirements.txt (line 4)) (21.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from pytest->-r requirements.txt (line 4)) (21.3)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /opt/conda/lib/python3.8/site-packages (from pytest->-r requirements.txt (line 4)) (0.13.1)\n",
      "Requirement already satisfied: toml in /opt/conda/lib/python3.8/site-packages (from pytest->-r requirements.txt (line 4)) (0.10.2)\n",
      "Requirement already satisfied: munch<3.0,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from sacred->-r requirements.txt (line 5)) (2.5.0)\n",
      "Requirement already satisfied: GitPython in /opt/conda/lib/python3.8/site-packages (from sacred->-r requirements.txt (line 5)) (3.1.27)\n",
      "Requirement already satisfied: py-cpuinfo>=4.0 in /opt/conda/lib/python3.8/site-packages (from sacred->-r requirements.txt (line 5)) (8.0.0)\n",
      "Requirement already satisfied: colorama>=0.4 in /opt/conda/lib/python3.8/site-packages (from sacred->-r requirements.txt (line 5)) (0.4.5)\n",
      "Requirement already satisfied: docopt<1.0,>=0.3 in /opt/conda/lib/python3.8/site-packages (from sacred->-r requirements.txt (line 5)) (0.6.2)\n",
      "Requirement already satisfied: jsonpickle<2.0,>=1.2 in /opt/conda/lib/python3.8/site-packages (from sacred->-r requirements.txt (line 5)) (1.5.2)\n",
      "Requirement already satisfied: wrapt<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from sacred->-r requirements.txt (line 5)) (1.12.1)\n",
      "Requirement already satisfied: neptune-client>=0.4.126 in /opt/conda/lib/python3.8/site-packages (from neptune-contrib->-r requirements.txt (line 7)) (0.16.4)\n",
      "Requirement already satisfied: Pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from neptune-contrib->-r requirements.txt (line 7)) (9.2.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from neptune-contrib->-r requirements.txt (line 7)) (3.3.4)\n",
      "Requirement already satisfied: attrdict>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from neptune-contrib->-r requirements.txt (line 7)) (2.0.1)\n",
      "Requirement already satisfied: joblib>=0.13 in /opt/conda/lib/python3.8/site-packages (from neptune-contrib->-r requirements.txt (line 7)) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->-r requirements.txt (line 11)) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 14)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 14)) (2022.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 15)) (0.8.0)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 15)) (2022.3.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning->-r requirements.txt (line 15)) (0.11.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from attrdict>=2.0.0->neptune-contrib->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 15)) (3.8.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 15)) (2.28.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->neptune-contrib->-r requirements.txt (line 7)) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->neptune-contrib->-r requirements.txt (line 7)) (1.4.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.8/site-packages (from matplotlib->neptune-contrib->-r requirements.txt (line 7)) (3.0.4)\n",
      "Requirement already satisfied: jsonschema<4.0.0 in /opt/conda/lib/python3.8/site-packages (from neptune-client>=0.4.126->neptune-contrib->-r requirements.txt (line 7)) (3.2.0)\n",
      "Requirement already satisfied: oauthlib>=2.1.0 in /opt/conda/lib/python3.8/site-packages (from neptune-client>=0.4.126->neptune-contrib->-r requirements.txt (line 7)) (3.2.0)\n",
      "Requirement already satisfied: bravado in /opt/conda/lib/python3.8/site-packages (from neptune-client>=0.4.126->neptune-contrib->-r requirements.txt (line 7)) (11.0.3)\n",
      "Requirement already satisfied: boto3>=1.16.0 in /opt/conda/lib/python3.8/site-packages (from neptune-client>=0.4.126->neptune-contrib->-r requirements.txt (line 7)) (1.21.21)\n",
      "Requirement already satisfied: PyJWT in /opt/conda/lib/python3.8/site-packages (from neptune-client>=0.4.126->neptune-contrib->-r requirements.txt (line 7)) (2.4.0)\n",
      "Requirement already satisfied: requests-oauthlib>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from neptune-client>=0.4.126->neptune-contrib->-r requirements.txt (line 7)) (1.3.1)\n",
      "Requirement already satisfied: swagger-spec-validator>=2.7.4 in /opt/conda/lib/python3.8/site-packages (from neptune-client>=0.4.126->neptune-contrib->-r requirements.txt (line 7)) (2.7.4)\n",
      "Requirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.8/site-packages (from neptune-client>=0.4.126->neptune-contrib->-r requirements.txt (line 7)) (0.18.2)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.8/site-packages (from neptune-client>=0.4.126->neptune-contrib->-r requirements.txt (line 7)) (8.1.3)\n",
      "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /opt/conda/lib/python3.8/site-packages (from neptune-client>=0.4.126->neptune-contrib->-r requirements.txt (line 7)) (0.58.0)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.8/site-packages (from neptune-client>=0.4.126->neptune-contrib->-r requirements.txt (line 7)) (1.26.11)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.8/site-packages (from GitPython->sacred->-r requirements.txt (line 5)) (4.0.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->torch>=1.5->-r requirements.txt (line 1)) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.8/site-packages (from sympy->torch>=1.5->-r requirements.txt (line 1)) (1.2.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3>=1.16.0->neptune-client>=0.4.126->neptune-contrib->-r requirements.txt (line 7)) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.25.0,>=1.24.21 in /opt/conda/lib/python3.8/site-packages (from boto3>=1.16.0->neptune-client>=0.4.126->neptune-contrib->-r requirements.txt (line 7)) (1.24.21)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from boto3>=1.16.0->neptune-client>=0.4.126->neptune-contrib->-r requirements.txt (line 7)) (0.5.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->GitPython->sacred->-r requirements.txt (line 5)) (5.0.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema<4.0.0->neptune-client>=0.4.126->neptune-contrib->-r requirements.txt (line 7)) (0.18.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 15)) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 15)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 15)) (3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 15)) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 15)) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 15)) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 15)) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 15)) (1.3.1)\n",
      "Requirement already satisfied: monotonic in /opt/conda/lib/python3.8/site-packages (from bravado->neptune-client>=0.4.126->neptune-contrib->-r requirements.txt (line 7)) (1.6)\n",
      "Requirement already satisfied: simplejson in /opt/conda/lib/python3.8/site-packages (from bravado->neptune-client>=0.4.126->neptune-contrib->-r requirements.txt (line 7)) (3.17.6)\n",
      "Requirement already satisfied: bravado-core>=5.16.1 in /opt/conda/lib/python3.8/site-packages (from bravado->neptune-client>=0.4.126->neptune-contrib->-r requirements.txt (line 7)) (5.17.0)\n",
      "Requirement already satisfied: msgpack in /opt/conda/lib/python3.8/site-packages (from bravado->neptune-client>=0.4.126->neptune-contrib->-r requirements.txt (line 7)) (1.0.3)\n",
      "Requirement already satisfied: jsonref in /opt/conda/lib/python3.8/site-packages (from bravado-core>=5.16.1->bravado->neptune-client>=0.4.126->neptune-contrib->-r requirements.txt (line 7)) (0.2)\n",
      "Requirement already satisfied: webcolors in /opt/conda/lib/python3.8/site-packages (from jsonschema<4.0.0->neptune-client>=0.4.126->neptune-contrib->-r requirements.txt (line 7)) (1.12)\n",
      "Requirement already satisfied: rfc3987 in /opt/conda/lib/python3.8/site-packages (from jsonschema<4.0.0->neptune-client>=0.4.126->neptune-contrib->-r requirements.txt (line 7)) (1.3.8)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.8/site-packages (from jsonschema<4.0.0->neptune-client>=0.4.126->neptune-contrib->-r requirements.txt (line 7)) (2.3)\n",
      "Requirement already satisfied: strict-rfc3339 in /opt/conda/lib/python3.8/site-packages (from jsonschema<4.0.0->neptune-client>=0.4.126->neptune-contrib->-r requirements.txt (line 7)) (0.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "smooth-weight",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdepres/miniconda3/envs/py39/lib/python3.9/site-packages/neptune/internal/backends/hosted_client.py:50: NeptuneDeprecationWarning: The 'neptune-client' package has been deprecated and will be removed in the future. Install the 'neptune' package instead. For more, see https://docs.neptune.ai/setup/upgrading/\n",
      "  from neptune.version import version as neptune_client_version\n",
      "/home/mdepres/miniconda3/envs/py39/lib/python3.9/site-packages/pytorch_lightning/loggers/neptune.py:39: NeptuneDeprecationWarning: You're importing the Neptune client library via the deprecated `neptune.new` module, which will be removed in a future release. Import directly from `neptune` instead.\n",
      "  from neptune import new as neptune\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from toolbox import losses\n",
    "from toolbox.losses import coloring_loss, triplet_loss\n",
    "from toolbox import metrics\n",
    "from loaders.loaders import siamese_loader\n",
    "from toolbox.metrics import all_losses_acc, accuracy_linear_assignment\n",
    "from toolbox.utils import check_dir\n",
    "from models import finetuning_models, get_siamese_model_test\n",
    "from models import utils\n",
    "from loaders import data_generator\n",
    "from loaders.data_generator import KCOL_Generator, QAP_Generator, MBS_Generator\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30d3d215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmdepres\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "junior-calvin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device_config(model_path):\n",
    "    \"\"\" Get the same device as used for training the pretrained model \"\"\"\n",
    "    config_file = os.path.join(model_path,'config.json')\n",
    "    with open(config_file) as json_file:\n",
    "        config_model = json.load(json_file)\n",
    "    use_cuda = not config_model['cpu'] and torch.cuda.is_available()\n",
    "    device = 'cuda' if use_cuda else 'cpu'\n",
    "    return config_model, device\n",
    "\n",
    "def compute_dataset(args,path_dataset,train=True,bs=10):\n",
    "    \"\"\" Computes some examples for the task\n",
    "        - args : 'data' part of the config file\n",
    "        - path_dataset : path to which the dataset will be saved, to use it several times\n",
    "        - train : True if this is a training dataset, False otherwise\n",
    "        - bs : desired batch size\"\"\"\n",
    "    num_batches = math.ceil(args['num_examples_val']/bs)\n",
    "    if train:\n",
    "        gene = MBS_Generator('train', args, path_dataset)\n",
    "    else:\n",
    "        gene = MBS_Generator('test', args, path_dataset)\n",
    "    gene.load_dataset()\n",
    "    loader = siamese_loader(gene, bs, gene.constant_n_vertices)\n",
    "    return loader\n",
    "\n",
    "def is_coloring(graph, coloring):\n",
    "    \"\"\" Computes the number of edges incompatible in the given coloring\n",
    "    This is used as a measure of accuract for the coloring problem \"\"\"\n",
    "    score = 0 #Number of contradictions\n",
    "    n_edges = 0\n",
    "    for i in range(len(graph)):\n",
    "        for j in range(len(graph)):\n",
    "            if graph[i][j]==1 :\n",
    "                n_edges+=1\n",
    "                if coloring[i]==coloring[j]:\n",
    "                    score+=1\n",
    "    return score/n_edges # Percentage of errors\n",
    "\n",
    "def train_epoch(model, embed_model, train_loader, loss_fn, optimizer):\n",
    "    \"\"\" Train the model for one epoch \n",
    "        - model : the model to be trained\n",
    "        - embed_model : the pretrained model, which has a node embedder\n",
    "        - train_loader : the dataset\n",
    "        - loss_fn : the loss function to use for this task\n",
    "        - optimizer : an initialized optimizer instance \"\"\"\n",
    "    model.train()\n",
    "    cum_loss = 0\n",
    "    for idx, (graph,tgt) in enumerate(train_loader):\n",
    "        graph['input'] = graph['input'].to(device)\n",
    "        \n",
    "        embed = embed_model.node_embedder(graph)['ne/suffix']\n",
    "        embed = torch.permute(embed,(0,2,1))\n",
    "        \n",
    "        inp = torch.cat((torch.permute(graph['input'],(0,2,3,1))[:,:,:,0],embed), 2)\n",
    "        \n",
    "        tgt = tgt['input'].type(torch.LongTensor).to(device)\n",
    "        \n",
    "        out = model(inp)\n",
    "        out = out.view((-1,out.shape[-1])) # The dependency of nodes to graph is irrelevant now\n",
    "        \n",
    "        tgt = tgt.view((-1,))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = loss_fn(out,tgt) #loss_fn(graph['input'], out, tgt)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        cum_loss += loss.item()\n",
    "    return cum_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def evaluate(model, embed_model, val_loader, compute_score=True):\n",
    "    \"\"\" Evaluate the model at some point in the training\n",
    "        - model : the model being trained\n",
    "        - embed_molde : the pretrained model whose embeddings we use\n",
    "        - val_loader : the dataset\n",
    "        (- compute_score : do we compute an accuracy Broken)\"\"\"\n",
    "    model.eval()\n",
    "    cum_loss = 0\n",
    "    cum_acc = 0\n",
    "    for idx, (graph, tgt) in (enumerate(valid_loader)):\n",
    "        graph['input'] = graph['input'].to(device)\n",
    "        embed = embed_model.node_embedder(graph)['ne/suffix']\n",
    "        embed = torch.permute(embed,(0,2,1))\n",
    "        \n",
    "        inp = torch.cat((torch.permute(graph['input'],(0,2,3,1))[:,:,:,0],embed), 2)\n",
    "        \n",
    "        tgt = tgt['input'].type(torch.LongTensor).to(device)\n",
    "        \n",
    "        out = model(inp)\n",
    "        out = out.view((-1,out.shape[-1]))\n",
    "        \n",
    "        tgt = tgt.view((-1,))\n",
    "        \n",
    "        loss = loss_fn(out, tgt)\n",
    "        cum_loss += loss.item()\n",
    "        \n",
    "        o = out.cpu().detach().numpy()\n",
    "        t = tgt.cpu().detach().numpy()\n",
    "        cum_acc += accuracy_score(np.argmax(o,axis=1),t)\n",
    "    print(cum_acc / len(val_loader))\n",
    "    return cum_loss / len(val_loader)\n",
    "\n",
    "def predict(model, embed_model, adj):\n",
    "    \"\"\" Show the result of the model on one example \"\"\"\n",
    "    graph = {'input':adj.to(device)}\n",
    "    embed = embed_model.node_embedder(graph)['ne/suffix']\n",
    "    embed = torch.permute(embed,(0,2,1))\n",
    "    \n",
    "    inp = torch.cat((torch.permute(graph['input'],(0,2,3,1))[:,:,:,0],embed), 2)\n",
    "    \n",
    "    out = model(inp)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-capital",
   "metadata": {},
   "source": [
    "## Loading the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "309bc105",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "#cwd = \"/\".join(cwd.split(\"/\")[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "answering-british",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Siamese_Node_Exp(\n",
       "  (node_embedder): Network(\n",
       "    (ne_bm_in): GraphNorm()\n",
       "    (ne_bm_block1_mlp3): MlpBlock_Real(\n",
       "      (convs): ModuleList(\n",
       "        (0): Conv2d(2, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1-2): 2 x Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (gn): GraphNorm()\n",
       "    )\n",
       "    (ne_bm_cat1): Concat()\n",
       "    (ne_bm_block2_mlp1): MlpBlock_Real(\n",
       "      (convs): ModuleList(\n",
       "        (0): Conv2d(66, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1-2): 2 x Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (gn): GraphNorm()\n",
       "    )\n",
       "    (ne_bm_block2_mlp2): MlpBlock_Real(\n",
       "      (convs): ModuleList(\n",
       "        (0): Conv2d(66, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1-2): 2 x Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (gn): GraphNorm()\n",
       "    )\n",
       "    (ne_bm_block2_mult): Matmul()\n",
       "    (ne_bm_block2_cat): Concat()\n",
       "    (ne_bm_block2_mlp3): MlpBlock_Real(\n",
       "      (convs): ModuleList(\n",
       "        (0): Conv2d(130, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1-2): 2 x Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (gn): GraphNorm()\n",
       "    )\n",
       "    (ne_bm_cat2): Concat()\n",
       "    (ne_bm_block3_mlp1): MlpBlock_Real(\n",
       "      (convs): ModuleList(\n",
       "        (0): Conv2d(66, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1-2): 2 x Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (gn): GraphNorm()\n",
       "    )\n",
       "    (ne_bm_block3_mlp2): MlpBlock_Real(\n",
       "      (convs): ModuleList(\n",
       "        (0): Conv2d(66, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1-2): 2 x Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (gn): GraphNorm()\n",
       "    )\n",
       "    (ne_bm_block3_mult): Matmul()\n",
       "    (ne_bm_block3_cat): Concat()\n",
       "    (ne_bm_block3_mlp3): MlpBlock_Real(\n",
       "      (convs): ModuleList(\n",
       "        (0): Conv2d(130, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1-2): 2 x Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (gn): GraphNorm()\n",
       "    )\n",
       "    (ne_bm_cat3): Concat()\n",
       "    (ne_bm_block4_mlp1): MlpBlock_Real(\n",
       "      (convs): ModuleList(\n",
       "        (0): Conv2d(66, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1-2): 2 x Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (gn): GraphNorm()\n",
       "    )\n",
       "    (ne_bm_block4_mlp2): MlpBlock_Real(\n",
       "      (convs): ModuleList(\n",
       "        (0): Conv2d(66, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1-2): 2 x Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (gn): GraphNorm()\n",
       "    )\n",
       "    (ne_bm_block4_mult): Matmul()\n",
       "    (ne_bm_block4_cat): Concat()\n",
       "    (ne_bm_block4_mlp3): MlpBlock_Real(\n",
       "      (convs): ModuleList(\n",
       "        (0): Conv2d(130, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1-2): 2 x Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (gn): GraphNorm()\n",
       "    )\n",
       "    (ne_suffix): ColumnMaxPooling()\n",
       "  )\n",
       "  (loss): triplet_loss(\n",
       "    (loss): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = cwd + '/qap/expe_new/node_embedding_rec_Regular_150_0.05/05-15-23-16-14'\n",
    "config_model, device = get_device_config(model_path)\n",
    "load_path = model_path + '/qap_expe_new/aqljez5s/checkpoints/epoch=9-step=6250.ckpt'\n",
    "embed_model = get_siamese_model_test(load_path) #config_model[\"data\"][\"test\"][\"path_model\"])\n",
    "embed_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-vocabulary",
   "metadata": {},
   "source": [
    "## Generating data and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6fc6156",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_model[\"data\"][\"test\"]['connection_density'] = 0.5\n",
    "config_model[\"data\"][\"test\"][\"num_examples_train\"] = 10000\n",
    "config_model[\"data\"][\"test\"][\"num_examples_val\"] = 100\n",
    "config_model[\"data\"][\"test\"][\"n_vertices\"] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "organizational-belarus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset at /home/mdepres/graph_neural_net/experiments-gnn/mbs/data/Color_ErdosRenyi_10000_50_1.0_0.05/train.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [03:47<00:00, 44.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset at /home/mdepres/graph_neural_net/experiments-gnn/mbs/data/Color_ErdosRenyi_10000_50_1.0_0.05/train.pkl\n",
      "Reading dataset at /home/mdepres/graph_neural_net/experiments-gnn/mbs/data/Color_ErdosRenyi_1000_50_1.0_0.05/test.pkl\n"
     ]
    }
   ],
   "source": [
    "args = config_model[\"data\"][\"test\"]\n",
    "train_loader = compute_dataset(args, cwd+'/experiments-gnn/mbs/data')\n",
    "valid_loader = compute_dataset(args, cwd+'/experiments-gnn/mbs/data', train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9acc7f",
   "metadata": {},
   "source": [
    "## Training a logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43806ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(loss='log_loss', warm_start=True)\n",
    "\n",
    "for idx, (graph,tgt) in enumerate(train_loader):\n",
    "    graph['input'] = graph['input'].to(device)\n",
    "    embed = embed_model.node_embedder(graph)['ne/suffix']\n",
    "    \n",
    "    embed = embed.cpu().detach().numpy()\n",
    "    embed = np.swapaxes(embed,1,2)\n",
    "    embed = np.resize(embed, (embed.shape[0]*embed.shape[1],embed.shape[-1]))\n",
    "    tgt = tgt['input']\n",
    "    tgt = np.resize(tgt, (tgt.shape[0]*tgt.shape[1],))\n",
    "    \n",
    "    clf.partial_fit(embed, tgt, classes=np.arange(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b574c39b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m         adj \u001b[38;5;241m=\u001b[39m graph[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m][e]\n\u001b[1;32m     17\u001b[0m         n_v \u001b[38;5;241m=\u001b[39m config_model[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_vertices\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 18\u001b[0m         edge_score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mis_coloring\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m[\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mn_v\u001b[49m\u001b[43m:\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mn_v\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     acc\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39maccuracy_score(pred,tgt)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(acc\u001b[38;5;241m/\u001b[39midx, edge_score\u001b[38;5;241m/\u001b[39midx)\n",
      "Cell \u001b[0;32mIn[36], line 32\u001b[0m, in \u001b[0;36mis_coloring\u001b[0;34m(graph, coloring)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(graph)):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(graph)):\n\u001b[0;32m---> 32\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m graph[i][j]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m :\n\u001b[1;32m     33\u001b[0m             n_edges\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     34\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m coloring[i]\u001b[38;5;241m==\u001b[39mcoloring[j]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "edge_score = 0\n",
    "\n",
    "for idx, (graph,tgt) in enumerate(valid_loader):\n",
    "    graph['input'] = graph['input'].to(device)\n",
    "    embed = embed_model.node_embedder(graph)['ne/suffix']\n",
    "    \n",
    "    embed = embed.cpu().detach().numpy()\n",
    "    embed = np.swapaxes(embed,1,2)    \n",
    "    embed = np.resize(embed, (embed.shape[0]*embed.shape[1],embed.shape[-1]))\n",
    "    tgt = tgt['input']\n",
    "    tgt = np.resize(tgt, (tgt.shape[0]*tgt.shape[1],))\n",
    "    \n",
    "    pred = clf.predict(embed)\n",
    "    for e in range(graph['input'].shape[0]): #For each graph in the batch\n",
    "        adj = graph['input'][e]\n",
    "        n_v = config_model[\"data\"][\"test\"][\"n_vertices\"]\n",
    "        edge_score += is_coloring(adj[0],pred[e*n_v:(e+1)*n_v])\n",
    "    acc+=accuracy_score(pred,tgt)\n",
    "\n",
    "print(acc/idx, edge_score/idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-virus",
   "metadata": {},
   "source": [
    "## Training the coloring model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caroline-bookmark",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierModel(\n",
       "  (mlp1): Linear(in_features=114, out_features=128, bias=True)\n",
       "  (mlp2): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = finetuning_models.ClassifierModel(args['n_vertices'],embed_dim=args['n_vertices']+64, k=2, hidden_dim = 128)#args[\"k\"])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "featured-tampa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5030000000000002\n",
      "Epoch: 1, Train loss: 346.681, Val loss: 346.591, Epoch time = 22.666s\n",
      "0.5058\n",
      "Epoch: 2, Train loss: 346.520, Val loss: 346.591, Epoch time = 22.176s\n",
      "0.5039000000000001\n",
      "Epoch: 3, Train loss: 346.444, Val loss: 346.612, Epoch time = 22.253s\n",
      "0.5037200000000002\n",
      "Epoch: 4, Train loss: 346.396, Val loss: 346.628, Epoch time = 22.627s\n",
      "0.50266\n",
      "Epoch: 5, Train loss: 346.375, Val loss: 346.685, Epoch time = 18.027s\n",
      "0.5023600000000001\n",
      "Epoch: 6, Train loss: 346.351, Val loss: 346.709, Epoch time = 16.754s\n",
      "0.5018400000000001\n",
      "Epoch: 7, Train loss: 346.333, Val loss: 346.694, Epoch time = 16.868s\n",
      "0.5009000000000002\n",
      "Epoch: 8, Train loss: 346.314, Val loss: 346.699, Epoch time = 17.286s\n",
      "0.5001999999999999\n",
      "Epoch: 9, Train loss: 346.301, Val loss: 346.741, Epoch time = 17.334s\n",
      "0.50046\n",
      "Epoch: 10, Train loss: 346.278, Val loss: 346.725, Epoch time = 16.818s\n",
      "0.5008399999999998\n",
      "Epoch: 11, Train loss: 346.267, Val loss: 346.698, Epoch time = 17.254s\n",
      "0.5001599999999997\n",
      "Epoch: 12, Train loss: 346.253, Val loss: 346.741, Epoch time = 17.240s\n",
      "0.5009600000000002\n",
      "Epoch: 13, Train loss: 346.233, Val loss: 346.747, Epoch time = 17.240s\n",
      "0.5005400000000001\n",
      "Epoch: 14, Train loss: 346.221, Val loss: 346.792, Epoch time = 17.273s\n",
      "0.5014000000000001\n",
      "Epoch: 15, Train loss: 346.195, Val loss: 346.756, Epoch time = 17.034s\n",
      "0.5006999999999999\n",
      "Epoch: 16, Train loss: 346.180, Val loss: 346.784, Epoch time = 16.609s\n",
      "0.5009\n",
      "Epoch: 17, Train loss: 346.155, Val loss: 346.804, Epoch time = 17.311s\n",
      "0.50058\n",
      "Epoch: 18, Train loss: 346.124, Val loss: 346.849, Epoch time = 17.271s\n",
      "0.5006599999999998\n",
      "Epoch: 19, Train loss: 346.107, Val loss: 346.812, Epoch time = 16.891s\n",
      "0.50044\n",
      "Epoch: 20, Train loss: 346.077, Val loss: 346.818, Epoch time = 16.563s\n",
      "0.5008400000000002\n",
      "Epoch: 21, Train loss: 346.052, Val loss: 346.847, Epoch time = 16.924s\n",
      "0.50064\n",
      "Epoch: 22, Train loss: 346.026, Val loss: 346.831, Epoch time = 17.164s\n",
      "0.5006600000000001\n",
      "Epoch: 23, Train loss: 346.000, Val loss: 346.846, Epoch time = 17.259s\n",
      "0.5011000000000001\n",
      "Epoch: 24, Train loss: 345.970, Val loss: 346.826, Epoch time = 16.894s\n",
      "0.4996000000000001\n",
      "Epoch: 25, Train loss: 345.936, Val loss: 346.839, Epoch time = 17.283s\n",
      "0.5000399999999998\n",
      "Epoch: 26, Train loss: 345.909, Val loss: 346.896, Epoch time = 17.338s\n",
      "0.49908\n",
      "Epoch: 27, Train loss: 345.869, Val loss: 346.892, Epoch time = 16.779s\n",
      "0.5008000000000002\n",
      "Epoch: 28, Train loss: 345.850, Val loss: 346.875, Epoch time = 16.917s\n",
      "0.4995800000000001\n",
      "Epoch: 29, Train loss: 345.815, Val loss: 346.908, Epoch time = 17.281s\n",
      "0.49827999999999983\n",
      "Epoch: 30, Train loss: 345.778, Val loss: 346.925, Epoch time = 17.299s\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "num_epochs = 30\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(model, embed_model, train_loader,loss_fn,optimizer)\n",
    "    end_time = time.time()\n",
    "    val_loss = evaluate(model, embed_model, valid_loader)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"\n",
    "          f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a45c493",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 104\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(model, embed_model, adj)\u001b[0m\n\u001b[1;32m    101\u001b[0m embed \u001b[38;5;241m=\u001b[39m embed_model\u001b[38;5;241m.\u001b[39mnode_embedder(graph)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mne/suffix\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    102\u001b[0m embed \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mpermute(embed,(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 104\u001b[0m inp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[:,:,:,\u001b[38;5;241m0\u001b[39m],embed), \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    106\u001b[0m out \u001b[38;5;241m=\u001b[39m model(inp)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 4"
     ]
    }
   ],
   "source": [
    "predict(model,embed_model,torch.tensor([[0.,1.,1.,0.,0.,0.], [1.,0.,1.,0.,0.,0.], [1.,1.,0.,0.,0.,0.], [0.,0.,0.,0.,1.,1.], [0.,0.,0.,1.,0.,1.],[0.,0.,0.,1.,1.,0.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f27159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = torch.tensor([[0.,1.,1.,0.,0.,0.],\n",
    "                    [1.,0.,0.,1.,0.,0.],\n",
    "                    [1.,0.,0.,0.,1.,0.],\n",
    "                    [0.,1.,0.,0.,0.,1.],\n",
    "                    [0.,0.,1.,0.,0.,1.],\n",
    "                    [0.,0.,0.,1.,1.,0.]])\n",
    "graph = {'input':adj.to(device)}\n",
    "a = embed_model.node_embedder(graph)['ne/suffix']\n",
    "#a = torch.permute(a,(0,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54325917",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = torch.tensor([[0.,1.,1.,0.,0.,0.],\n",
    "                    [1.,0.,1.,0.,0.,0.],\n",
    "                    [1.,1.,0.,0.,0.,0.],\n",
    "                    [0.,0.,0.,0.,1.,1.],\n",
    "                    [0.,0.,0.,1.,0.,1.],\n",
    "                    [0.,0.,0.,1.,1.,0.]])\n",
    "graph = {'input':adj.to(device)}\n",
    "b = embed_model.node_embedder(graph)['ne/suffix']\n",
    "#b = torch.permute(b,(0,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f98721c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(427.0795, device='cuda:0', grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "844d6ce1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4402,  0.4402,  0.4402,  0.4402,  0.4402,  0.4402],\n",
       "         [ 0.4009,  0.4009,  0.4009,  0.4010,  0.4009,  0.4009],\n",
       "         [ 0.4204,  0.4204,  0.4204,  0.4204,  0.4204,  0.4204],\n",
       "         [ 0.0847,  0.0847,  0.0847,  0.0847,  0.0847,  0.0847],\n",
       "         [ 0.3063,  0.3063,  0.3063,  0.3063,  0.3063,  0.3063],\n",
       "         [ 0.0539,  0.0539,  0.0539,  0.0539,  0.0539,  0.0539],\n",
       "         [ 0.2470,  0.2470,  0.2470,  0.2470,  0.2470,  0.2470],\n",
       "         [ 0.5839,  0.5839,  0.5839,  0.5839,  0.5839,  0.5839],\n",
       "         [ 0.2048,  0.2048,  0.2048,  0.2048,  0.2048,  0.2048],\n",
       "         [ 0.5894,  0.5894,  0.5894,  0.5894,  0.5894,  0.5894],\n",
       "         [ 0.2026,  0.2026,  0.2026,  0.2026,  0.2026,  0.2026],\n",
       "         [ 0.2651,  0.2651,  0.2651,  0.2651,  0.2651,  0.2651],\n",
       "         [ 0.3780,  0.3780,  0.3780,  0.3780,  0.3780,  0.3780],\n",
       "         [ 0.0116,  0.0116,  0.0116,  0.0116,  0.0116,  0.0116],\n",
       "         [ 0.1653,  0.1653,  0.1653,  0.1653,  0.1653,  0.1653],\n",
       "         [ 0.3254,  0.3254,  0.3254,  0.3254,  0.3254,  0.3254],\n",
       "         [ 0.0799,  0.0799,  0.0799,  0.0799,  0.0799,  0.0799],\n",
       "         [ 0.5202,  0.5202,  0.5202,  0.5202,  0.5202,  0.5202],\n",
       "         [ 0.3711,  0.3711,  0.3711,  0.3711,  0.3711,  0.3711],\n",
       "         [ 0.0814,  0.0814,  0.0814,  0.0814,  0.0814,  0.0814],\n",
       "         [ 0.1746,  0.1746,  0.1746,  0.1746,  0.1746,  0.1746],\n",
       "         [ 0.0498,  0.0498,  0.0498,  0.0498,  0.0498,  0.0498],\n",
       "         [ 0.3443,  0.3443,  0.3443,  0.3443,  0.3443,  0.3443],\n",
       "         [ 0.5375,  0.5375,  0.5375,  0.5375,  0.5375,  0.5375],\n",
       "         [ 0.3995,  0.3995,  0.3995,  0.3995,  0.3995,  0.3995],\n",
       "         [ 0.3669,  0.3669,  0.3669,  0.3669,  0.3669,  0.3669],\n",
       "         [ 0.2667,  0.2667,  0.2667,  0.2667,  0.2667,  0.2667],\n",
       "         [ 0.2096,  0.2096,  0.2096,  0.2096,  0.2096,  0.2096],\n",
       "         [ 0.0930,  0.0930,  0.0930,  0.0930,  0.0930,  0.0930],\n",
       "         [ 0.4147,  0.4148,  0.4148,  0.4147,  0.4148,  0.4147],\n",
       "         [ 0.2899,  0.2899,  0.2899,  0.2899,  0.2899,  0.2899],\n",
       "         [ 0.4520,  0.4520,  0.4520,  0.4520,  0.4520,  0.4520],\n",
       "         [ 0.3902,  0.3902,  0.3902,  0.3902,  0.3902,  0.3902],\n",
       "         [ 0.2523,  0.2523,  0.2523,  0.2523,  0.2523,  0.2523],\n",
       "         [ 0.4802,  0.4802,  0.4802,  0.4802,  0.4802,  0.4802],\n",
       "         [ 0.1030,  0.1030,  0.1030,  0.1030,  0.1030,  0.1030],\n",
       "         [ 0.2714,  0.2714,  0.2714,  0.2714,  0.2714,  0.2714],\n",
       "         [ 0.1695,  0.1695,  0.1695,  0.1695,  0.1695,  0.1695],\n",
       "         [ 0.3449,  0.3449,  0.3449,  0.3449,  0.3449,  0.3449],\n",
       "         [ 0.3565,  0.3565,  0.3565,  0.3565,  0.3565,  0.3565],\n",
       "         [ 0.1701,  0.1701,  0.1701,  0.1701,  0.1701,  0.1701],\n",
       "         [ 0.3794,  0.3794,  0.3794,  0.3794,  0.3794,  0.3794],\n",
       "         [ 0.5168,  0.5168,  0.5168,  0.5168,  0.5168,  0.5168],\n",
       "         [ 0.2819,  0.2819,  0.2819,  0.2819,  0.2819,  0.2819],\n",
       "         [ 0.2309,  0.2308,  0.2308,  0.2308,  0.2308,  0.2308],\n",
       "         [ 0.0234,  0.0234,  0.0234,  0.0234,  0.0234,  0.0234],\n",
       "         [ 0.4434,  0.4434,  0.4434,  0.4434,  0.4434,  0.4434],\n",
       "         [ 0.4536,  0.4536,  0.4536,  0.4536,  0.4536,  0.4536],\n",
       "         [ 0.2278,  0.2278,  0.2278,  0.2278,  0.2278,  0.2278],\n",
       "         [ 0.2384,  0.2384,  0.2384,  0.2384,  0.2384,  0.2384],\n",
       "         [ 0.2380,  0.2380,  0.2380,  0.2380,  0.2380,  0.2380],\n",
       "         [-0.0458, -0.0458, -0.0458, -0.0458, -0.0458, -0.0458],\n",
       "         [ 0.3440,  0.3440,  0.3440,  0.3440,  0.3440,  0.3440],\n",
       "         [ 0.0198,  0.0198,  0.0198,  0.0198,  0.0198,  0.0198],\n",
       "         [ 0.1149,  0.1149,  0.1149,  0.1149,  0.1149,  0.1149],\n",
       "         [ 0.1227,  0.1227,  0.1227,  0.1227,  0.1227,  0.1227],\n",
       "         [ 0.2037,  0.2037,  0.2037,  0.2037,  0.2037,  0.2037],\n",
       "         [ 0.2193,  0.2193,  0.2193,  0.2193,  0.2193,  0.2193],\n",
       "         [ 0.2510,  0.2510,  0.2510,  0.2510,  0.2510,  0.2510],\n",
       "         [-0.0093, -0.0093, -0.0093, -0.0093, -0.0093, -0.0093],\n",
       "         [ 0.3251,  0.3251,  0.3251,  0.3251,  0.3251,  0.3251],\n",
       "         [ 0.2746,  0.2746,  0.2746,  0.2746,  0.2746,  0.2746],\n",
       "         [ 0.1861,  0.1861,  0.1861,  0.1861,  0.1861,  0.1861],\n",
       "         [ 0.2039,  0.2039,  0.2039,  0.2039,  0.2039,  0.2039]]],\n",
       "       device='cuda:0', grad_fn=<MaxBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c89767a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5876,  0.3406,  0.2466,  0.2042,  0.5165,  0.3910,  0.4041,\n",
      "           0.2596,  0.3764,  0.2289,  0.5486,  0.2175,  0.2678,  0.4765,\n",
      "           0.1607,  0.5217,  0.2615,  0.0721,  0.0830,  0.4413,  0.5251,\n",
      "           0.2456,  0.2534,  0.5228,  0.3429,  0.1875,  0.5983,  0.6452,\n",
      "           0.5067,  0.3438,  0.2812,  0.2997,  0.3568,  0.5723,  0.3633,\n",
      "           0.1703,  0.5524,  0.3218,  0.3659,  0.7511,  0.1907,  0.4892,\n",
      "           0.4102,  0.3257,  0.3389,  0.3946,  0.4401,  0.1967,  0.2710,\n",
      "           0.3492,  0.3172,  0.4290,  0.3768,  0.1926,  0.6328,  0.2215,\n",
      "           0.2491,  0.3007,  0.3100,  0.3219,  0.3196,  0.1653,  0.3739,\n",
      "           0.3907],\n",
      "         [ 0.2488,  0.7574,  0.1916,  0.0957,  0.2733,  0.2073,  0.4112,\n",
      "           0.4048, -0.1448,  0.5541,  0.0497,  0.0351,  0.2117, -0.0638,\n",
      "           0.0870, -0.0973, -0.1115,  0.7538,  0.1979,  0.1233, -0.0699,\n",
      "          -0.0465,  0.4094,  0.0853,  0.3693,  0.7197,  0.2446,  0.1500,\n",
      "           0.0173,  0.5344,  0.6372,  0.8484,  0.2652,  0.2147,  0.3374,\n",
      "           0.0425,  0.0432,  0.1030,  0.1476,  0.0464,  0.2432,  0.1922,\n",
      "           0.6376,  0.4291,  0.5548, -0.2549,  0.4125,  0.3044,  0.1113,\n",
      "           0.0593,  0.2308, -0.0455,  0.2592, -0.0393,  0.0417,  0.5563,\n",
      "          -0.1235,  0.3254, -0.0604, -0.3107,  0.3669,  0.7899,  0.5362,\n",
      "           0.3713],\n",
      "         [ 0.5876,  0.3406,  0.2466,  0.2042,  0.5165,  0.3910,  0.4041,\n",
      "           0.2596,  0.3764,  0.2289,  0.5486,  0.2175,  0.2678,  0.4765,\n",
      "           0.1607,  0.5217,  0.2615,  0.0721,  0.0830,  0.4413,  0.5251,\n",
      "           0.2456,  0.2534,  0.5228,  0.3429,  0.1875,  0.5983,  0.6452,\n",
      "           0.5067,  0.3438,  0.2812,  0.2997,  0.3568,  0.5723,  0.3633,\n",
      "           0.1703,  0.5524,  0.3218,  0.3659,  0.7511,  0.1907,  0.4892,\n",
      "           0.4102,  0.3257,  0.3389,  0.3946,  0.4401,  0.1967,  0.2710,\n",
      "           0.3492,  0.3172,  0.4290,  0.3768,  0.1926,  0.6328,  0.2215,\n",
      "           0.2491,  0.3007,  0.3100,  0.3219,  0.3196,  0.1653,  0.3739,\n",
      "           0.3907],\n",
      "         [ 0.5876,  0.3406,  0.2466,  0.2042,  0.5165,  0.3910,  0.4041,\n",
      "           0.2596,  0.3764,  0.2289,  0.5486,  0.2175,  0.2678,  0.4765,\n",
      "           0.1607,  0.5217,  0.2615,  0.0721,  0.0830,  0.4413,  0.5251,\n",
      "           0.2456,  0.2534,  0.5228,  0.3429,  0.1875,  0.5983,  0.6452,\n",
      "           0.5067,  0.3438,  0.2812,  0.2997,  0.3568,  0.5723,  0.3633,\n",
      "           0.1703,  0.5524,  0.3218,  0.3659,  0.7511,  0.1907,  0.4892,\n",
      "           0.4102,  0.3257,  0.3389,  0.3946,  0.4401,  0.1967,  0.2710,\n",
      "           0.3492,  0.3172,  0.4290,  0.3768,  0.1926,  0.6328,  0.2215,\n",
      "           0.2491,  0.3007,  0.3100,  0.3219,  0.3196,  0.1653,  0.3739,\n",
      "           0.3907],\n",
      "         [ 0.2488,  0.7574,  0.1916,  0.0957,  0.2733,  0.2073,  0.4112,\n",
      "           0.4048, -0.1448,  0.5541,  0.0497,  0.0351,  0.2117, -0.0638,\n",
      "           0.0870, -0.0973, -0.1115,  0.7538,  0.1979,  0.1233, -0.0698,\n",
      "          -0.0465,  0.4094,  0.0853,  0.3693,  0.7197,  0.2446,  0.1500,\n",
      "           0.0173,  0.5344,  0.6372,  0.8484,  0.2652,  0.2147,  0.3374,\n",
      "           0.0425,  0.0432,  0.1030,  0.1476,  0.0464,  0.2432,  0.1922,\n",
      "           0.6376,  0.4291,  0.5548, -0.2549,  0.4125,  0.3044,  0.1113,\n",
      "           0.0593,  0.2308, -0.0455,  0.2592, -0.0393,  0.0417,  0.5563,\n",
      "          -0.1235,  0.3254, -0.0604, -0.3107,  0.3669,  0.7899,  0.5362,\n",
      "           0.3713],\n",
      "         [ 0.5876,  0.3406,  0.2466,  0.2042,  0.5165,  0.3910,  0.4041,\n",
      "           0.2596,  0.3764,  0.2289,  0.5486,  0.2175,  0.2678,  0.4765,\n",
      "           0.1607,  0.5217,  0.2615,  0.0721,  0.0830,  0.4413,  0.5251,\n",
      "           0.2456,  0.2534,  0.5228,  0.3429,  0.1875,  0.5983,  0.6452,\n",
      "           0.5067,  0.3438,  0.2812,  0.2997,  0.3568,  0.5723,  0.3633,\n",
      "           0.1703,  0.5524,  0.3218,  0.3659,  0.7511,  0.1907,  0.4892,\n",
      "           0.4102,  0.3257,  0.3389,  0.3946,  0.4401,  0.1967,  0.2710,\n",
      "           0.3492,  0.3172,  0.4290,  0.3768,  0.1926,  0.6328,  0.2215,\n",
      "           0.2491,  0.3007,  0.3100,  0.3219,  0.3196,  0.1653,  0.3739,\n",
      "           0.3907]]], device='cuda:0', grad_fn=<PermuteBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2034, 0.1985, 0.2054, 0.1984, 0.1943],\n",
       "         [0.1971, 0.1998, 0.1937, 0.1870, 0.2223],\n",
       "         [0.2034, 0.1985, 0.2054, 0.1984, 0.1943],\n",
       "         [0.2034, 0.1985, 0.2054, 0.1984, 0.1943],\n",
       "         [0.1971, 0.1998, 0.1937, 0.1870, 0.2223],\n",
       "         [0.2034, 0.1985, 0.2054, 0.1984, 0.1943]]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj = torch.tensor([[0.,1.,0.,1.,0.,0.],[1.,0.,1.,0.,1.,0.],[0.,1.,0.,0.,0.,1.],[1.,0.,0.,0.,1.,0.],[0.,1.,0.,1.,0.,1.],[0.,0.,1.,0.,1.,0.]])\n",
    "predict(model, embed_model, adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09c91a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.7032,  0.3343,  0.1633,  0.0436,  0.2701,  0.2554,  0.5439,\n",
      "           0.3833,  0.1126,  0.3754,  0.3926,  0.3669,  0.2555,  0.1962,\n",
      "           0.1400,  0.6513,  0.1217,  0.4270,  0.3237,  0.0689,  0.2727,\n",
      "           0.0443,  0.3573,  0.3288,  0.3606,  0.6120,  0.4097,  0.4621,\n",
      "           0.1649,  0.3616,  0.2090,  0.1824,  0.1069,  0.4411,  0.4770,\n",
      "           0.2129,  0.3177,  0.1633,  0.3647,  0.6151,  0.2132,  0.2559,\n",
      "           0.1412,  0.3616,  0.1514,  0.0670,  0.4375,  0.2516,  0.2437,\n",
      "           0.4261,  0.2835,  0.0991, -0.0378, -0.1193,  0.2556, -0.0353,\n",
      "           0.1390,  0.1086,  0.3103,  0.1045,  0.4721,  0.3369,  0.2279,\n",
      "           0.1597],\n",
      "         [ 0.3299,  0.3688,  0.1374,  0.0162,  0.7938, -0.1448,  0.2472,\n",
      "           0.7531,  0.2155,  0.8976, -0.0659,  0.2481,  0.8007,  0.0072,\n",
      "           0.2777,  0.0511,  0.0214,  0.5763, -0.1206,  0.0088,  0.1456,\n",
      "           0.0097,  0.1737,  0.2144, -0.2511,  0.2920,  0.3336,  0.2075,\n",
      "           0.0551,  0.6862,  0.5766,  1.0172,  0.8582, -0.0450,  0.3649,\n",
      "           0.0713,  0.3305,  0.3349,  0.4531,  0.2400,  0.1906,  0.3420,\n",
      "           0.9769,  0.1412,  0.3202, -0.0734,  0.3523,  0.1696,  0.2562,\n",
      "           0.1772,  0.3498,  0.0442,  0.9127,  0.6027, -0.0159,  0.2036,\n",
      "           0.3319,  0.6496,  0.2760, -0.0741,  0.2350,  0.0199,  0.2030,\n",
      "           0.5509],\n",
      "         [ 0.3299,  0.3688,  0.1374,  0.0162,  0.7938, -0.1448,  0.2472,\n",
      "           0.7531,  0.2155,  0.8976, -0.0659,  0.2481,  0.8007,  0.0072,\n",
      "           0.2777,  0.0511,  0.0214,  0.5763, -0.1206,  0.0088,  0.1456,\n",
      "           0.0097,  0.1737,  0.2144, -0.2511,  0.2920,  0.3336,  0.2075,\n",
      "           0.0551,  0.6862,  0.5766,  1.0172,  0.8582, -0.0450,  0.3649,\n",
      "           0.0713,  0.3305,  0.3349,  0.4531,  0.2400,  0.1906,  0.3420,\n",
      "           0.9769,  0.1412,  0.3202, -0.0734,  0.3523,  0.1696,  0.2562,\n",
      "           0.1772,  0.3498,  0.0442,  0.9127,  0.6027, -0.0159,  0.2036,\n",
      "           0.3319,  0.6496,  0.2760, -0.0741,  0.2350,  0.0199,  0.2030,\n",
      "           0.5509],\n",
      "         [ 0.7032,  0.3343,  0.1633,  0.0436,  0.2701,  0.2554,  0.5439,\n",
      "           0.3833,  0.1126,  0.3754,  0.3926,  0.3669,  0.2555,  0.1962,\n",
      "           0.1400,  0.6513,  0.1217,  0.4270,  0.3237,  0.0689,  0.2727,\n",
      "           0.0443,  0.3573,  0.3288,  0.3606,  0.6120,  0.4097,  0.4621,\n",
      "           0.1649,  0.3616,  0.2090,  0.1824,  0.1069,  0.4411,  0.4770,\n",
      "           0.2129,  0.3177,  0.1633,  0.3647,  0.6151,  0.2132,  0.2559,\n",
      "           0.1412,  0.3616,  0.1514,  0.0670,  0.4375,  0.2516,  0.2437,\n",
      "           0.4261,  0.2835,  0.0991, -0.0378, -0.1193,  0.2556, -0.0353,\n",
      "           0.1390,  0.1086,  0.3103,  0.1045,  0.4721,  0.3369,  0.2279,\n",
      "           0.1597],\n",
      "         [ 0.7032,  0.3343,  0.1633,  0.0436,  0.2701,  0.2554,  0.5439,\n",
      "           0.3833,  0.1126,  0.3754,  0.3926,  0.3669,  0.2555,  0.1962,\n",
      "           0.1400,  0.6513,  0.1217,  0.4270,  0.3237,  0.0689,  0.2727,\n",
      "           0.0443,  0.3573,  0.3288,  0.3606,  0.6120,  0.4097,  0.4621,\n",
      "           0.1649,  0.3616,  0.2090,  0.1824,  0.1069,  0.4411,  0.4770,\n",
      "           0.2129,  0.3177,  0.1633,  0.3647,  0.6151,  0.2132,  0.2559,\n",
      "           0.1412,  0.3616,  0.1514,  0.0670,  0.4375,  0.2516,  0.2437,\n",
      "           0.4261,  0.2835,  0.0991, -0.0378, -0.1193,  0.2556, -0.0353,\n",
      "           0.1390,  0.1086,  0.3103,  0.1045,  0.4721,  0.3369,  0.2279,\n",
      "           0.1597],\n",
      "         [ 0.7032,  0.3343,  0.1633,  0.0436,  0.2701,  0.2554,  0.5439,\n",
      "           0.3833,  0.1126,  0.3754,  0.3926,  0.3669,  0.2555,  0.1962,\n",
      "           0.1400,  0.6513,  0.1217,  0.4270,  0.3237,  0.0689,  0.2727,\n",
      "           0.0443,  0.3573,  0.3288,  0.3606,  0.6120,  0.4097,  0.4621,\n",
      "           0.1649,  0.3616,  0.2090,  0.1824,  0.1069,  0.4411,  0.4770,\n",
      "           0.2129,  0.3177,  0.1633,  0.3647,  0.6151,  0.2132,  0.2559,\n",
      "           0.1412,  0.3616,  0.1514,  0.0670,  0.4375,  0.2516,  0.2437,\n",
      "           0.4261,  0.2835,  0.0991, -0.0378, -0.1193,  0.2556, -0.0353,\n",
      "           0.1390,  0.1086,  0.3103,  0.1045,  0.4721,  0.3369,  0.2279,\n",
      "           0.1597]]], device='cuda:0', grad_fn=<PermuteBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2019, 0.1972, 0.1998, 0.1991, 0.2021],\n",
       "         [0.1813, 0.2067, 0.1981, 0.2048, 0.2090],\n",
       "         [0.1813, 0.2067, 0.1981, 0.2048, 0.2090],\n",
       "         [0.2019, 0.1972, 0.1998, 0.1991, 0.2021],\n",
       "         [0.2019, 0.1972, 0.1998, 0.1991, 0.2021],\n",
       "         [0.2019, 0.1972, 0.1998, 0.1991, 0.2021]]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj = torch.tensor([[0.,1.,0.,0.,1.,0.],[1.,0.,1.,0.,1.,0.],[0.,1.,0.,1.,0.,1.],[0.,0.,1.,0.,0.,1.],[1.,1.,0.,0.,0.,0.],[0.,0.,1.,1.,0.,0.]])\n",
    "predict(model, embed_model, adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e34516",
   "metadata": {},
   "source": [
    "# Training for coloring from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6c153a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mdepres/miniconda3/envs/py39/lib/python3.9/site-packages/neptune/internal/backends/hosted_client.py:50: NeptuneDeprecationWarning: The 'neptune-client' package has been deprecated and will be removed in the future. Install the 'neptune' package instead. For more, see https://docs.neptune.ai/setup/upgrading/\n",
      "  from neptune.version import version as neptune_client_version\n",
      "/home/mdepres/miniconda3/envs/py39/lib/python3.9/site-packages/pytorch_lightning/loggers/neptune.py:39: NeptuneDeprecationWarning: You're importing the Neptune client library via the deprecated `neptune.new` module, which will be removed in a future release. Import directly from `neptune` instead.\n",
      "  from neptune import new as neptune\n",
      "[rank: 0] Global seed set to 3787\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mdepres/graph_neural_net/commander_explore.py\", line 339, in <module>\n",
      "    main()\n",
      "  File \"/home/mdepres/graph_neural_net/commander_explore.py\", line 303, in main\n",
      "    config = get_config(args.config)\n",
      "  File \"/home/mdepres/graph_neural_net/commander_explore.py\", line 22, in get_config\n",
      "    config = yaml.safe_load(f)\n",
      "  File \"/home/mdepres/miniconda3/envs/py39/lib/python3.9/site-packages/yaml/__init__.py\", line 125, in safe_load\n",
      "    return load(stream, SafeLoader)\n",
      "  File \"/home/mdepres/miniconda3/envs/py39/lib/python3.9/site-packages/yaml/__init__.py\", line 81, in load\n",
      "    return loader.get_single_data()\n",
      "  File \"/home/mdepres/miniconda3/envs/py39/lib/python3.9/site-packages/yaml/constructor.py\", line 49, in get_single_data\n",
      "    node = self.get_single_node()\n",
      "  File \"/home/mdepres/miniconda3/envs/py39/lib/python3.9/site-packages/yaml/composer.py\", line 36, in get_single_node\n",
      "    document = self.compose_document()\n",
      "  File \"/home/mdepres/miniconda3/envs/py39/lib/python3.9/site-packages/yaml/composer.py\", line 55, in compose_document\n",
      "    node = self.compose_node(None, None)\n",
      "  File \"/home/mdepres/miniconda3/envs/py39/lib/python3.9/site-packages/yaml/composer.py\", line 84, in compose_node\n",
      "    node = self.compose_mapping_node(anchor)\n",
      "  File \"/home/mdepres/miniconda3/envs/py39/lib/python3.9/site-packages/yaml/composer.py\", line 133, in compose_mapping_node\n",
      "    item_value = self.compose_node(node, item_key)\n",
      "  File \"/home/mdepres/miniconda3/envs/py39/lib/python3.9/site-packages/yaml/composer.py\", line 84, in compose_node\n",
      "    node = self.compose_mapping_node(anchor)\n",
      "  File \"/home/mdepres/miniconda3/envs/py39/lib/python3.9/site-packages/yaml/composer.py\", line 133, in compose_mapping_node\n",
      "    item_value = self.compose_node(node, item_key)\n",
      "  File \"/home/mdepres/miniconda3/envs/py39/lib/python3.9/site-packages/yaml/composer.py\", line 84, in compose_node\n",
      "    node = self.compose_mapping_node(anchor)\n",
      "  File \"/home/mdepres/miniconda3/envs/py39/lib/python3.9/site-packages/yaml/composer.py\", line 127, in compose_mapping_node\n",
      "    while not self.check_event(MappingEndEvent):\n",
      "  File \"/home/mdepres/miniconda3/envs/py39/lib/python3.9/site-packages/yaml/parser.py\", line 98, in check_event\n",
      "    self.current_event = self.state()\n",
      "  File \"/home/mdepres/miniconda3/envs/py39/lib/python3.9/site-packages/yaml/parser.py\", line 428, in parse_block_mapping_key\n",
      "    if self.check_token(KeyToken):\n",
      "  File \"/home/mdepres/miniconda3/envs/py39/lib/python3.9/site-packages/yaml/scanner.py\", line 116, in check_token\n",
      "    self.fetch_more_tokens()\n",
      "  File \"/home/mdepres/miniconda3/envs/py39/lib/python3.9/site-packages/yaml/scanner.py\", line 258, in fetch_more_tokens\n",
      "    raise ScannerError(\"while scanning for the next token\", None,\n",
      "yaml.scanner.ScannerError: while scanning for the next token\n",
      "found character '\\t' that cannot start any token\n",
      "  in \"default_config.yaml\", line 20, column 1\n"
     ]
    }
   ],
   "source": [
    "!python3 commander_explore.py train --n_vertices 150 --noise 0.2 --edge_density 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe7301e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
